{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(ì—­í•  ë¶€ì—¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì „ ì„¤ì¹˜ : pip install ollama\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì±—ë´‡ì˜ ê¸°ë³¸ì  ì§ˆë¬¸, ë‹µë³€ ì—­í• \n",
    "def ask_gemma(question):\n",
    "    # ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µ ìƒì„±\n",
    "    chatbot_role = \"You are a helpful assistant.\"\n",
    "    response = ollama.chat(model='gemma2', messages=[\n",
    "        {\"role\": \"system\", \"content\": chatbot_role},  # ì±—ë´‡ì˜ ê¸°ë³¸ ì—­í•  ë¶€ì—¬\n",
    "        {\"role\": \"user\", \"content\": question}, # ì§ˆë¬¸\n",
    "    ])\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¸ê³„ì—ì„œ ê°€ì¥ ë†’ì€ ì‚°ì€ **ì—ë² ë ˆìŠ¤íŠ¸ì‚°**ì…ë‹ˆë‹¤. í•´ë°œ 8,848.86m(29,031.7í”¼íŠ¸)ë¡œ, ë„¤íŒ”ê³¼ ì¤‘êµ­ ì‚¬ì´ì˜ ê²½ê³„ì„  ìœ„ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "\n",
      "ë„ì›€ì´ ë˜ì…¨ê¸°ë¥¼ ë°”ëë‹ˆë‹¤! ğŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"ì„¸ê³„ì—ì„œ ê°€ì¥ ë†’ì€ ì‚°ì„ ì•Œë ¤ì¤˜.\"\n",
    "response = ask_gemma(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(Gradio ì‚¬ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì „ ì„¤ì¹˜ : pip install gradio\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_1732\\3441953186.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì±„íŒ… ê¸°ë¡ì„ í¬í•¨í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "def chat(message, history):\n",
    "    # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ChatOllama í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append(HumanMessage(content=human))\n",
    "        chat_history.append(AIMessage(content=ai))\n",
    "\n",
    "    # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "\n",
    "    # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = model.invoke(chat_history)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"ì•ˆë…•í•˜ì„¸ìš”!\",\n",
    "        \"ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "        \"íŒŒì´ì¬ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "    ],\n",
    "    title=\"AI ì±—ë´‡\",\n",
    "    description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€í•©ë‹ˆë‹¤.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì„œë²„ ì‹¤í–‰\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(Gradio + csv ì‚¬ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV íŒŒì¼ ë¡œë“œ\n",
    "df = pd.read_csv(\"./dataset/indata_kor.csv\", encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df.to_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_1732\\2819514520.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Ai_Project\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOllama(model=\"gemma2\", tempeature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vectorstore.as_retriever(search_kwargs={\"k\":1}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì±„íŒ… í•¨ìˆ˜ ì •ì˜\n",
    "def chat(message, history):\n",
    "    # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ConversationalRetrievalChain í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    chat_history = [(human, ai) for human, ai in history]\n",
    "\n",
    "    # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
    "\n",
    "    # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ\n",
    "    sources = set([doc.metadata.get('source', 'Unknown') for doc in response['source_documents']])\n",
    "    source_info = f\"\\n\\nì°¸ê³  ì¶œì²˜: {', '.join(sources)}\" if sources else \"\"\n",
    "\n",
    "    return response['answer'] + source_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"í•œêµ­í´ë¦¬í…ëŒ€í•™ ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼ ë©´ì ‘ì‹œì—ëŠ” ì–´ë–¤ê±¸ ì¤€ë¹„í•˜ê³  ê°€ë©´ ë ê¹Œìš”?\",\n",
    "        \"ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”\",\n",
    "        \"í•œêµ­í´ë¦¬í…ëŒ€í•œ ì¶”ì²œí• ë§Œí•œ í•™ê³¼ í•˜ë‚˜ë¥¼ ì†Œê°œí•´ì£¼ì„¸ìš”.\"\n",
    "    ],\n",
    "    title=\"ëŒ€í•™ ì •ë³´ AI ì±—ë´‡\",\n",
    "    description=\"ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ CSVë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ í•œê¸€ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_1732\\4160252103.py:7: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n"
     ]
    }
   ],
   "source": [
    "# ì„œë²„ ì‹¤í–‰\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(ì¸í„°ë„· URLì •ë³´ ìš”ì•½í•˜ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from wordcloud import WordCloud\n",
    "from dotenv import load_dotenv\n",
    "from gtts import gTTS\n",
    "import tempfile\n",
    "import whisper\n",
    "import ollama\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + r\"C:/ffmpeg/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
    "    try:\n",
    "        transcribed_text = transcribe_audio(audio)\n",
    "        return transcribed_text\n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, split, and retrieve documents\n",
    "\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"gemma2\")\n",
    "\n",
    "    # vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format documents\n",
    "\n",
    "def format_docs(docs):\n",
    "\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that defines the RAG chain\n",
    "\n",
    "def rag_chain(url, question):\n",
    "\n",
    "    retriever = load_and_retrieve_docs(url)\n",
    "\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "\n",
    "    response = ollama.chat(model='gemma2', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(text):\n",
    "    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='malgun',\n",
    "        background_color='white',\n",
    "        width=800,\n",
    "        height=600,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        min_font_size=10,\n",
    "        random_state=42\n",
    "    ).generate(text)\n",
    "    \n",
    "    # í”Œë¡¯ ìƒì„± ë° figure ê°ì²´ ë°˜í™˜\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gradio interface\n",
    "\n",
    "# iface = gr.Interface(\n",
    "\n",
    "#     fn=rag_chain,\n",
    "\n",
    "#     inputs=[\"text\", \"text\"],\n",
    "\n",
    "#     outputs=\"text\",\n",
    "\n",
    "#     title=\"RAG Chain Question Answering\",\n",
    "\n",
    "#     description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_1732\\2629661401.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"gemma2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # ë””ë²„ê·¸ ëª¨ë“œë¡œ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "# iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get text from URL\n",
    "def get_text_from_url(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang='ko'):\n",
    "    # ì„ì‹œ íŒŒì¼ ìƒì„±\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:\n",
    "        temp_filename = fp.name\n",
    "\n",
    "    # TTS ë³€í™˜\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ì¬ìƒ ë˜ëŠ” ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Tabbed Interface\n",
    "with gr.Blocks() as iface:\n",
    "    # Tab for Question and Answer\n",
    "    with gr.Tab(\"ì§ˆë¬¸ê³¼ ë‹µë³€\"):\n",
    "        gr.Interface(\n",
    "            fn=rag_chain,\n",
    "            inputs=[\"text\", \"text\"],\n",
    "            outputs=\"text\",\n",
    "            title=\"RAG Chain Question Answering\",\n",
    "            description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "        ).render()\n",
    "\n",
    "    with gr.Tab(\"ì‹œê°í™” (ì›Œë“œí´ë¼ìš°ë“œ)\"):\n",
    "        with gr.Row():\n",
    "            text_input = gr.Textbox(\n",
    "                label=\"URL ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥\",\n",
    "                placeholder=\"URLì´ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”\",\n",
    "                lines=5\n",
    "            )\n",
    "        \n",
    "        def process_input(input_text):\n",
    "            if input_text.startswith(('http://', 'https://')):\n",
    "                text = get_text_from_url(input_text)\n",
    "            else:\n",
    "                text = input_text\n",
    "            return generate_wordcloud(text)\n",
    "        \n",
    "        plot_output = gr.Plot()  # Plot ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©\n",
    "        generate_btn = gr.Button(\"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\")\n",
    "        \n",
    "        generate_btn.click(\n",
    "            fn=process_input,\n",
    "            inputs=text_input,\n",
    "            outputs=plot_output\n",
    "        )\n",
    "    with gr.Tab(\"ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"):\n",
    "        gr.Interface(\n",
    "            fn=process_audio,\n",
    "            inputs=gr.Audio(type=\"filepath\", label=\"MP3 íŒŒì¼ ì—…ë¡œë“œ\"),\n",
    "            outputs = \"text\",\n",
    "            title = \"MP3 to Text Converter\",\n",
    "            description = \"MP3 íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\n",
    "        )\n",
    "    with gr.Tab(\"í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜\"):\n",
    "        gr.Interface(\n",
    "            fn=process_tts,\n",
    "            inputs=[\n",
    "                gr.Textbox(lines=5, label=\"í…ìŠ¤íŠ¸ ì…ë ¥\"),\n",
    "                gr.Dropdown(choices=['ko','en','ja','zh-cn'], label=\"ì–¸ì–´ ì„ íƒ\", value='ko')\n",
    "            ],\n",
    "            outputs=[\n",
    "                gr.Audio(label=\"ìƒì„±ëœ ì˜¤ë””ì˜¤\"),\n",
    "                gr.Textbox(label=\"ìƒíƒœ ë©”ì‹œì§€\")\n",
    "            ],\n",
    "            title = \"Text to Speech Converter\",\n",
    "            description = \"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ MP3íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë””ë²„ê·¸ ëª¨ë“œë¡œ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(STT:ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ì „í™˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import whisper\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffmpeg ê²½ë¡œ ëª…ì‹œì  ì„¤ì •\n",
    "# os.environ[\"FFMPEG_BINARY\"] = \"C:/aiproject/ffmpeg/bin/ffmpeg.exe\"\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:/ffmpeg/bin\"\n",
    "# os.environ[\"FFMPEG_BINARY\"] = r\"C:/Ai_Project/ffmpeg/bin/ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    # Whisper ëª¨ë¸ ë¡œë“œ\n",
    "    model = whisper.load_model(\"base\")\n",
    "    \n",
    "    # ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬\n",
    "    result = model.transcribe(audio_path)\n",
    "    \n",
    "    # ì „ì‚¬ëœ í…ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
    "    try:\n",
    "        transcribed_text = transcribe_audio(audio)\n",
    "        return transcribed_text\n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio, \n",
    "    inputs = gr.Audio(type=\"filepath\", label=\"MP3 íŒŒì¼ ì—…ë¡œë“œ\"),\n",
    "    outputs=\"text\", \n",
    "    title = \"MP3 to Text Converter\",\n",
    "    description = \"MP3 íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë””ë²„ê·¸ ëª¨ë“œë¡œ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(TTS:í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ë³€í™˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì „ ì„¤ì¹˜ : pip install gtts\n",
    "import gradio as gr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang='ko'):\n",
    "    # ì„ì‹œ íŒŒì¼ ìƒì„±\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:\n",
    "        temp_filename = fp.name\n",
    "    \n",
    "    # TTS ë³€í™˜\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ì¬ìƒ ë˜ëŠ” ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "iface = gr.Interface(\n",
    "    fn=process_tts, \n",
    "    inputs = [\n",
    "        gr.Textbox(lines=5, label=\"í…ìŠ¤íŠ¸ ì…ë ¥\"),\n",
    "        gr.Dropdown(choices=['ko','en','ja','zh-cn'], label=\"ì–¸ì–´ ì„ íƒ\", value='ko')\n",
    "    ], \n",
    "    outputs=[\n",
    "        gr.Audio(label=\"ìƒì„ ëœ ì˜¤ë””ì˜¤\"),\n",
    "        gr.Textbox(label=\"ìƒíƒœ ë©”ì‹œì§€\")\n",
    "    ],\n",
    "    title=\"Text to Speech Converter\",\n",
    "    description=\"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ MP3 íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë””ë²„ê·¸ ëª¨ë“œë¡œ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
