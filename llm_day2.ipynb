{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(역할 부여)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install ollama\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇의 기본적 질문, 답변 역할\n",
    "def ask_gemma(question):\n",
    "    # ollama를 사용하여 모델로부터 응답 생성\n",
    "    chatbot_role = \"You are a helpful assistant.\"\n",
    "    response = ollama.chat(model='gemma2', messages=[\n",
    "        {\"role\": \"system\", \"content\": chatbot_role},  # 챗봇의 기본 역할 부여\n",
    "        {\"role\": \"user\", \"content\": question}, # 질문\n",
    "    ])\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "세계에서 가장 높은 산은 **에베레스트산**입니다. 해발 8,848.86m(29,031.7피트)로, 네팔과 중국 사이의 경계선 위에 위치해 있습니다.  \n",
      "\n",
      "\n",
      "도움이 되셨기를 바랍니다! 😊\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"세계에서 가장 높은 산을 알려줘.\"\n",
    "response = ask_gemma(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(Gradio 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install gradio\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_12124\\3441953186.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 기록을 포함하여 응답을 생성하는 함수\n",
    "def chat(message, history):\n",
    "    # 이전 대화 기록을 ChatOllama 형식으로 변환\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append(HumanMessage(content=human))\n",
    "        chat_history.append(AIMessage(content=ai))\n",
    "\n",
    "    # 현재 메시지 추가\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "\n",
    "    # 모델을 사용하여 응답 생성\n",
    "    response = model.invoke(chat_history)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"안녕하세요!\",\n",
    "        \"인공지능에 대해 설명해주세요.\",\n",
    "        \"파이썬의 장점은 무엇인가요?\"\n",
    "    ],\n",
    "    title=\"AI 챗봇\",\n",
    "    description=\"질문을 입력하면 AI가 답변합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(Gradio + csv 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 로드\n",
    "df = pd.read_csv(\"./dataset/indata_kor.csv\", encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df.to_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_12124\\2819514520.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Ai_Project\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 초기화\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 데이터베이스 생성\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "llm = ChatOllama(model=\"gemma2\", tempeature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vectorstore.as_retriever(search_kwargs={\"k\":1}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 함수 정의\n",
    "def chat(message, history):\n",
    "    # 이전 대화 기록을 ConversationalRetrievalChain 형식으로 변환\n",
    "    chat_history = [(human, ai) for human, ai in history]\n",
    "\n",
    "    # 모델을 사용하여 응답 생성\n",
    "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
    "\n",
    "    # 소스 문서 정보 추출\n",
    "    sources = set([doc.metadata.get('source', 'Unknown') for doc in response['source_documents']])\n",
    "    source_info = f\"\\n\\n참고 출처: {', '.join(sources)}\" if sources else \"\"\n",
    "\n",
    "    return response['answer'] + source_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?\",\n",
    "        \"스마트금융과에 대해 설명해주세요\",\n",
    "        \"한국폴리텍대한 추천할만한 학과 하나를 소개해주세요.\"\n",
    "    ],\n",
    "    title=\"대학 정보 AI 챗봇\",\n",
    "    description=\"스마트금융과에 대한 질문을 입력하면 AI가 CSV데이터를 참고하여 한글로 답변합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_12124\\4160252103.py:7: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n"
     ]
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(인터넷 URL정보 요약하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from wordcloud import WordCloud\n",
    "from dotenv import load_dotenv\n",
    "from gtts import gTTS\n",
    "import tempfile\n",
    "import whisper\n",
    "import ollama\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + r\"C:/ffmpeg/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"오디오 파일을 업로드해주세요.\"\n",
    "    try:\n",
    "        transcribed_text = transcribe_audio(audio)\n",
    "        return transcribed_text\n",
    "    except Exception as e:\n",
    "        return f\"오류가 발생했습니다.: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, split, and retrieve documents\n",
    "\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"gemma2\")\n",
    "\n",
    "    # vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format documents\n",
    "\n",
    "def format_docs(docs):\n",
    "\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that defines the RAG chain\n",
    "\n",
    "def rag_chain(url, question):\n",
    "\n",
    "    retriever = load_and_retrieve_docs(url)\n",
    "\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "\n",
    "    response = ollama.chat(model='gemma2', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(text):\n",
    "    # 워드클라우드 생성\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='malgun',\n",
    "        background_color='white',\n",
    "        width=800,\n",
    "        height=600,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        min_font_size=10,\n",
    "        random_state=42\n",
    "    ).generate(text)\n",
    "    \n",
    "    # 플롯 생성 및 figure 객체 반환\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gradio interface\n",
    "\n",
    "# iface = gr.Interface(\n",
    "\n",
    "#     fn=rag_chain,\n",
    "\n",
    "#     inputs=[\"text\", \"text\"],\n",
    "\n",
    "#     outputs=\"text\",\n",
    "\n",
    "#     title=\"RAG Chain Question Answering\",\n",
    "\n",
    "#     description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_1732\\2629661401.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"gemma2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 디버그 모드로 Gradio 인터페이스 실행\n",
    "# iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get text from URL\n",
    "def get_text_from_url(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang='ko'):\n",
    "    # 임시 파일 생성\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:\n",
    "        temp_filename = fp.name\n",
    "\n",
    "    # TTS 변환\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"텍스트를 입력해주세요.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"변환이 완료되었습니다. 아래에서 재생 또는 다운로드할 수 있습니다.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"오류가 발생했습니다: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Tabbed Interface\n",
    "with gr.Blocks() as iface:\n",
    "    # Tab for Question and Answer\n",
    "    with gr.Tab(\"질문과 답변\"):\n",
    "        gr.Interface(\n",
    "            fn=rag_chain,\n",
    "            inputs=[\"text\", \"text\"],\n",
    "            outputs=\"text\",\n",
    "            title=\"RAG Chain Question Answering\",\n",
    "            description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "        ).render()\n",
    "\n",
    "    with gr.Tab(\"시각화 (워드클라우드)\"):\n",
    "        with gr.Row():\n",
    "            text_input = gr.Textbox(\n",
    "                label=\"URL 또는 텍스트 입력\",\n",
    "                placeholder=\"URL이나 텍스트를 입력하세요\",\n",
    "                lines=5\n",
    "            )\n",
    "        \n",
    "        def process_input(input_text):\n",
    "            if input_text.startswith(('http://', 'https://')):\n",
    "                text = get_text_from_url(input_text)\n",
    "            else:\n",
    "                text = input_text\n",
    "            return generate_wordcloud(text)\n",
    "        \n",
    "        plot_output = gr.Plot()  # Plot 컴포넌트 사용\n",
    "        generate_btn = gr.Button(\"워드클라우드 생성\")\n",
    "        \n",
    "        generate_btn.click(\n",
    "            fn=process_input,\n",
    "            inputs=text_input,\n",
    "            outputs=plot_output\n",
    "        )\n",
    "    with gr.Tab(\"음성을 텍스트로 변환\"):\n",
    "        gr.Interface(\n",
    "            fn=process_audio,\n",
    "            inputs=gr.Audio(type=\"filepath\", label=\"MP3 파일 업로드\"),\n",
    "            outputs = \"text\",\n",
    "            title = \"MP3 to Text Converter\",\n",
    "            description = \"MP3 파일을 업로드하면 텍스트로 변환합니다.\"\n",
    "        )\n",
    "    with gr.Tab(\"텍스트를 음성으로 변환\"):\n",
    "        gr.Interface(\n",
    "            fn=process_tts,\n",
    "            inputs=[\n",
    "                gr.Textbox(lines=5, label=\"텍스트 입력\"),\n",
    "                gr.Dropdown(choices=['ko','en','ja','zh-cn'], label=\"언어 선택\", value='ko')\n",
    "            ],\n",
    "            outputs=[\n",
    "                gr.Audio(label=\"생성된 오디오\"),\n",
    "                gr.Textbox(label=\"상태 메시지\")\n",
    "            ],\n",
    "            title = \"Text to Speech Converter\",\n",
    "            description = \"텍스트를 입력하면 MP3파일로 변환합니다.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(STT:음성을 텍스트로 전환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import whisper\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffmpeg 경로 명시적 설정\n",
    "# os.environ[\"FFMPEG_BINARY\"] = \"C:/aiproject/ffmpeg/bin/ffmpeg.exe\"\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:/ffmpeg/bin\"\n",
    "# os.environ[\"FFMPEG_BINARY\"] = r\"C:/Ai_Project/ffmpeg/bin/ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    # Whisper 모델 로드\n",
    "    model = whisper.load_model(\"base\")\n",
    "    \n",
    "    # 오디오 파일 전사\n",
    "    result = model.transcribe(audio_path)\n",
    "    \n",
    "    # 전사된 텍스트 반환\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"오디오 파일을 업로드해주세요.\"\n",
    "    try:\n",
    "        transcribed_text = transcribe_audio(audio)\n",
    "        return transcribed_text\n",
    "    except Exception as e:\n",
    "        return f\"오류가 발생했습니다: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio, \n",
    "    inputs = gr.Audio(type=\"filepath\", label=\"MP3 파일 업로드\"),\n",
    "    outputs=\"text\", \n",
    "    title = \"MP3 to Text Converter\",\n",
    "    description = \"MP3 파일을 업로드하면 텍스트로 변환합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(TTS:텍스트를 음성변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install gtts\n",
    "import gradio as gr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang='ko'):\n",
    "    # 임시 파일 생성\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:\n",
    "        temp_filename = fp.name\n",
    "    \n",
    "    # TTS 변환\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"텍스트를 입력해주세요.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"변환이 완료되었습니다. 아래에서 재생 또는 다운로드할 수 있습니다.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"오류가 발생했습니다.: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=process_tts, \n",
    "    inputs = [\n",
    "        gr.Textbox(lines=5, label=\"텍스트 입력\"),\n",
    "        gr.Dropdown(choices=['ko','en','ja','zh-cn'], label=\"언어 선택\", value='ko')\n",
    "    ], \n",
    "    outputs=[\n",
    "        gr.Audio(label=\"생선된 오디오\"),\n",
    "        gr.Textbox(label=\"상태 메시지\")\n",
    "    ],\n",
    "    title=\"Text to Speech Converter\",\n",
    "    description=\"텍스트를 입력하면 MP3 파일로 변환합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DB접속 챗봇 예제(Gradio 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import re\n",
    "from typing import Dict, Any\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB 연결 설정\n",
    "DB_URL = \"mysql+pymysql://root:00000000@localhost:3306/test\"\n",
    "engine = create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedQueryGenerator:\n",
    "    \"\"\"향상된 SQL 쿼리 생성 클래스\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.query_template = \"\"\"\n",
    "        당신은 한국어를 잘하고 MySQL 데이터베이스의 쿼리를 생성하는 전문가입니다.\n",
    "        데이터베이스 스키마 정보:\n",
    "        {schema_info}\n",
    "\n",
    "        이전 피드백 정보:\n",
    "        {feedback_info}\n",
    "\n",
    "        위 정보를 바탕으로 다음 질문에 대한 MySQL 쿼리를 생성해주세요.\n",
    "        질문: {question}\n",
    "\n",
    "        규칙:\n",
    "        1. 순수한 SQL 쿼리만 작성하세요\n",
    "        2. 컬럼의 실제 값을 기준으로 쿼리를 작성하세요\n",
    "        3. 설명이나 주석을 포함하지 마세요\n",
    "        4. 쿼리는 SELECT 문으로 시작하고 세미콜론(;)으로 끝나야 합니다\n",
    "        5. WHERE 절에서는 정확한 값 매칭을 위해 = 연산자를 사용하세요\n",
    "        6. 유사 검색이 필요한 경우 LIKE '%키워드%' 를 사용하세요\n",
    "        7. 관련된 모든 결과를 찾기 위해 적절히 OR 조건을 활용하세요\n",
    "        \"\"\"\n",
    "\n",
    "        self.answer_template = \"\"\"\n",
    "        다음 정보를 바탕으로 사용자의 질문에 대한 답변을 생성해주세요:\n",
    "\n",
    "        원래 질문: {question}\n",
    "        실행된 쿼리: {query}\n",
    "        쿼리 결과: {result}\n",
    "\n",
    "        규칙:\n",
    "        1. 결과를 자연스러운 한국어로 설명해주세요\n",
    "        2. 숫자 데이터가 있다면 적절한 단위와 함께 표현해주세요\n",
    "        3. 결과가 없다면 그 이유를 설명해주세요\n",
    "        4. 전문적인 용어는 쉽게 풀어서 설명해주세요\n",
    "        \"\"\"\n",
    "\n",
    "        # Gemma2 모델 초기화\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.llm = Ollama(\n",
    "            model=\"gemma2\",\n",
    "            temperature=0,\n",
    "            callback_manager=callback_manager\n",
    "        )\n",
    "\n",
    "        # 프롬프트 템플릿 설정\n",
    "        self.query_prompt = ChatPromptTemplate.from_template(self.query_template)\n",
    "        self.answer_prompt = ChatPromptTemplate.from_template(self.answer_template)\n",
    "\n",
    "        # Chain 설정\n",
    "        self.query_chain = LLMChain(llm=self.llm, prompt=self.query_prompt)\n",
    "        self.answer_chain = LLMChain(llm=self.llm, prompt=self.answer_prompt)\n",
    "\n",
    "    def generate_query(self, question: str, schema_info: str, feedback_info: str = \"\") -> str:\n",
    "        \"\"\"질문에 대한 SQL 쿼리를 생성합니다.\"\"\"\n",
    "        response = self.query_chain.run(\n",
    "            question=question,\n",
    "            schema_info=schema_info,\n",
    "            feedback_info=feedback_info\n",
    "        )\n",
    "        return self.extract_sql_query(response)\n",
    "\n",
    "    def generate_answer(self, question: str, query: str, result: Any) -> str:\n",
    "        \"\"\"쿼리 결과를 바탕으로 자연어 답변을 생성합니다.\"\"\"\n",
    "        result_str = str(result) if isinstance(result, pd.DataFrame) else json.dumps(result, ensure_ascii=False)\n",
    "        response = self.answer_chain.run(\n",
    "            question=question,\n",
    "            query=query,\n",
    "            result=result_str\n",
    "        )\n",
    "        return response.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_sql_query(response: str) -> str:\n",
    "        \"\"\"응답에서 SQL 쿼리를 추출합니다.\"\"\"\n",
    "        response = response.replace('```sql', '').replace('```', '').strip()\n",
    "        match = re.search(r'SELECT.*?;', response, re.DOTALL | re.IGNORECASE)\n",
    "        return match.group(0).strip() if match else response.strip()\n",
    "\n",
    "# 쿼리 결과 반환\n",
    "def get_schema_info():\n",
    "    \"\"\"데이터베이스 스키마 정보를 가져옵니다.\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        tables = pd.read_sql(\"SHOW TABLES\", conn)\n",
    "        schema_info = []\n",
    "\n",
    "        for table in tables.iloc[:, 0]:\n",
    "            columns = pd.read_sql(f\"DESCRIBE {table}\", conn)\n",
    "            schema_info.append(f\"테이블: {table}\")\n",
    "            schema_info.append(\"컬럼:\")\n",
    "            for _, row in columns.iterrows():\n",
    "                schema_info.append(f\"- {row['Field']} ({row['Type']})\")\n",
    "            schema_info.append(\"\")\n",
    "\n",
    "        return \"\\n\".join(schema_info)\n",
    "\n",
    "def execute_query(query):\n",
    "    \"\"\"SQL 쿼리를 실행하고 결과를 반환합니다.\"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = pd.read_sql(query, conn)\n",
    "            return result\n",
    "    except Exception as e:\n",
    "        return f\"쿼리 실행 중 오류 발생: {str(e)}\"\n",
    "\n",
    "def process_question(question):\n",
    "    \"\"\"질문을 처리하고 결과를 반환합니다.\"\"\"\n",
    "    schema_info = get_schema_info()\n",
    "    query_generator = EnhancedQueryGenerator()\n",
    "\n",
    "    # 쿼리 생성\n",
    "    query = query_generator.generate_query(question, schema_info)\n",
    "\n",
    "    # 쿼리 실행\n",
    "    result = execute_query(query)\n",
    "\n",
    "    # 답변 생성\n",
    "    answer = query_generator.generate_answer(question, query, result)\n",
    "\n",
    "    return query, result, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "def create_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# DB 문의 챗봇 (Gemma2 기반)\")\n",
    "\n",
    "        with gr.Row():\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"질문을 입력하세요\",\n",
    "                placeholder=\"데이터베이스에 대해 궁금한 점을 물어보세요...\"\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"질문하기\")\n",
    "\n",
    "        with gr.Row():\n",
    "            query_output = gr.Textbox(label=\"생성된 SQL 쿼리\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                result_output = gr.Dataframe(label=\"쿼리 실행 결과\")\n",
    "\n",
    "        with gr.Row():\n",
    "            answer_output = gr.Textbox(\n",
    "                label=\"AI 답변\",\n",
    "                lines=5\n",
    "            )\n",
    "\n",
    "        submit_btn.click(\n",
    "            fn=process_question,\n",
    "            inputs=[question_input],\n",
    "            outputs=[query_output, result_output, answer_output]\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://1726dcbd9b81d7994f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1726dcbd9b81d7994f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7861 <> https://1726dcbd9b81d7994f.gradio.live\n"
     ]
    }
   ],
   "source": [
    "# 인터페이스 실행\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_interface()\n",
    "    demo.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자기소개서 도우미 챗봇 예제(Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_4824\\1062899569.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  ollama_model = Ollama(model = \"gemma2\")\n"
     ]
    }
   ],
   "source": [
    "# Ollama 설정 (Gemma2 모델 사용)\n",
    "os.environ[\"OLLAMA_API_BASE\"] = \"http://localhost:11434\" # Ollama 서버 주소\n",
    "ollama_model = Ollama(model = \"gemma2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 템플릿 설정\n",
    "TEMPLATES = {\n",
    "    \"취업\": \"Based on the following keywords and example, write a personal statement for a job application.\",\n",
    "    \"대학원\": \"Using the provided keywords, draft a personal statement for a graduate school application.\",\n",
    "    \"봉사활동\": \"With the given keywords, write a personal statement emphasizing volunteer experience and motivation.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어 지원: 한국어, 영어, 일본어\n",
    "LANGUAGES = {\n",
    "    \"한국어\": \"Please write the response in Korean.\",\n",
    "    \"영어\": \"Please write the response in English.\",\n",
    "    \"일본어\": \"Please write the response in Japanese.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 키워드 추천 함수\n",
    "def recommend_keywords(purpose):\n",
    "    if purpose == \"취업\":\n",
    "        return \"책임감, 팀워크, 문제 해결 능력\"\n",
    "    elif purpose == \"대학원\":\n",
    "        return \"연구 열정, 창의력, 학업 성취도\"\n",
    "    elif purpose == \"봉사활동\":\n",
    "        return \"사회적 책임감, 희생정신, 리더십\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자기소개서 작성 함수\n",
    "def generate_statement(purpose, language, keywords, example_sentence=None):\n",
    "    if purpose not in TEMPLATES:\n",
    "        return \"❌ 지원 목적을 올바르게 선택해주세요.\"\n",
    "    if language not in LANGUAGES:\n",
    "        return \"❌ 언어를 올바르게 선택해주세요.\"\n",
    "    \n",
    "    # 템플릿 생성\n",
    "    template = TEMPLATES[purpose] + \"\\n\\nKeywords: {keywords}\\n\" + LANGUAGES[language]\n",
    "    if example_sentence:\n",
    "        template += f\"\\n\\nExample sentence: {example_sentence}\"\n",
    "        \n",
    "    prompt =PromptTemplate(input_variables=[\"keywords\"], template = template)\n",
    "    chain = LLMChain(llm=ollama_model, prompt=prompt)\n",
    "    response = chain.run({\"keywords\": keywords})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 저장 함수\n",
    "def save_to_pdf(statement, filename=\"personal_statement.pdf\"):\n",
    "    pdf=FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.add_font('MalgunGothic', '', r'C:\\Windows\\Fonts\\malgun.ttf', uni=True) # '맑은 고딕' 폰트 경로 설정\n",
    "    pdf.set_font('MalgunGothic', size=12) # 폰트 설정\n",
    "    pdf.multi_cell(0,10,statement)\n",
    "    pdf.output(filename)\n",
    "    return f\"✔️ PDF 저장 완료: {filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스\n",
    "def chatbot_interface(purpose, language, keywords, example_sentence=None, save_pdf=False):\n",
    "    statement = generate_statement(purpose, language, keywords, example_sentence)\n",
    "    if save_pdf:\n",
    "        save_to_pdf(statement)\n",
    "    return statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 📝 다목적 자기소개서 작성 도우미\")\n",
    "    gr.Markdown(\"키워드와 추천 문장을 활용하여 취업, 대학원, 봉사활동 자기소개서를 생성하고 PDF로 저장하세요!\")\n",
    "    \n",
    "    # 입력 영역\n",
    "    with gr.Row():\n",
    "        purpose_input = gr.Dropdown(label=\"지원 목적\", choices=[\"취업\",\"대학원\",\"봉사활동\"], value=\"취업\")\n",
    "        language_input = gr.Dropdown(label=\"언어 선택\", choices=[\"한국어\", \"영어\", \"일본어\"], value=\"한국어\")\n",
    "        \n",
    "    recommended_keywords = gr.Textbox(label=\"추천 키워드\", interactive=False)\n",
    "    recommend_btn = gr.Button(\"키워드 추천\")\n",
    "    recommend_btn.click(recommend_keywords, inputs=[purpose_input], outputs=[recommended_keywords])\n",
    "    \n",
    "    with gr.Row():\n",
    "        keywords_input = gr.Textbox(label=\"사용자 키워드 입력\", placeholder=\"예: 책임감, 팀워크, 문제 해결 능력\")\n",
    "        example_sentence_input = gr.Textbox(\n",
    "            label=\"추천 문장 (선택 사항)\", \n",
    "            placeholder=\"예: '저는 도전을 두려워하지 않고 성공적으로 프로젝트를 완수했습니다.'\"\n",
    "        )\n",
    "    save_pdf_toggle=gr.Checkbox(label=\"PDF로 저장\", value=False)\n",
    "    \n",
    "    # 출력 영역\n",
    "    output = gr.Textbox(label=\"작성된 자기소개서\", lines=6)\n",
    "    submit_btn = gr.Button(\"작성하기\")\n",
    "    submit_btn.click(\n",
    "        fn=chatbot_interface, \n",
    "        inputs=[purpose_input, language_input, keywords_input, example_sentence_input, save_pdf_toggle],\n",
    "        outputs=[output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
