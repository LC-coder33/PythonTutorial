{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(ì—­í•  ë¶€ì—¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì „ ì„¤ì¹˜ : pip install ollama\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì±—ë´‡ì˜ ê¸°ë³¸ì  ì§ˆë¬¸, ë‹µë³€ ì—­í• \n",
    "def ask_gemma(question):\n",
    "    # ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µ ìƒì„±\n",
    "    chatbot_role = \"You are a helpful assistant.\"\n",
    "    response = ollama.chat(model='gemma2', messages=[\n",
    "        {\"role\": \"system\", \"content\": chatbot_role},  # ì±—ë´‡ì˜ ê¸°ë³¸ ì—­í•  ë¶€ì—¬\n",
    "        {\"role\": \"user\", \"content\": question}, # ì§ˆë¬¸\n",
    "    ])\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¸ê³„ì—ì„œ ê°€ì¥ ë†’ì€ ì‚°ì€ **ì—ë² ë ˆìŠ¤íŠ¸ì‚°**ì…ë‹ˆë‹¤. í•´ë°œ 8,848.86m(29,031.7í”¼íŠ¸)ë¡œ, ë„¤íŒ”ê³¼ ì¤‘êµ­ ì‚¬ì´ì˜ ê²½ê³„ì„  ìœ„ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "\n",
      "ë„ì›€ì´ ë˜ì…¨ê¸°ë¥¼ ë°”ëë‹ˆë‹¤! ğŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"ì„¸ê³„ì—ì„œ ê°€ì¥ ë†’ì€ ì‚°ì„ ì•Œë ¤ì¤˜.\"\n",
    "response = ask_gemma(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(Gradio ì‚¬ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì „ ì„¤ì¹˜ : pip install gradio\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_12124\\3441953186.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì±„íŒ… ê¸°ë¡ì„ í¬í•¨í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "def chat(message, history):\n",
    "    # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ChatOllama í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append(HumanMessage(content=human))\n",
    "        chat_history.append(AIMessage(content=ai))\n",
    "\n",
    "    # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "\n",
    "    # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = model.invoke(chat_history)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"ì•ˆë…•í•˜ì„¸ìš”!\",\n",
    "        \"ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "        \"íŒŒì´ì¬ì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "    ],\n",
    "    title=\"AI ì±—ë´‡\",\n",
    "    description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€í•©ë‹ˆë‹¤.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì„œë²„ ì‹¤í–‰\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(Gradio + csv ì‚¬ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV íŒŒì¼ ë¡œë“œ\n",
    "df = pd.read_csv(\"./dataset/indata_kor.csv\", encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df.to_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_12124\\2819514520.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Ai_Project\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOllama(model=\"gemma2\", tempeature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vectorstore.as_retriever(search_kwargs={\"k\":1}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì±„íŒ… í•¨ìˆ˜ ì •ì˜\n",
    "def chat(message, history):\n",
    "    # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ConversationalRetrievalChain í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    chat_history = [(human, ai) for human, ai in history]\n",
    "\n",
    "    # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
    "\n",
    "    # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ\n",
    "    sources = set([doc.metadata.get('source', 'Unknown') for doc in response['source_documents']])\n",
    "    source_info = f\"\\n\\nì°¸ê³  ì¶œì²˜: {', '.join(sources)}\" if sources else \"\"\n",
    "\n",
    "    return response['answer'] + source_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"í•œêµ­í´ë¦¬í…ëŒ€í•™ ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼ ë©´ì ‘ì‹œì—ëŠ” ì–´ë–¤ê±¸ ì¤€ë¹„í•˜ê³  ê°€ë©´ ë ê¹Œìš”?\",\n",
    "        \"ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”\",\n",
    "        \"í•œêµ­í´ë¦¬í…ëŒ€í•œ ì¶”ì²œí• ë§Œí•œ í•™ê³¼ í•˜ë‚˜ë¥¼ ì†Œê°œí•´ì£¼ì„¸ìš”.\"\n",
    "    ],\n",
    "    title=\"ëŒ€í•™ ì •ë³´ AI ì±—ë´‡\",\n",
    "    description=\"ìŠ¤ë§ˆíŠ¸ê¸ˆìœµê³¼ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ CSVë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ í•œê¸€ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_12124\\4160252103.py:7: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n"
     ]
    }
   ],
   "source": [
    "# ì„œë²„ ì‹¤í–‰\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(ì¸í„°ë„· URLì •ë³´ ìš”ì•½í•˜ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from wordcloud import WordCloud\n",
    "from dotenv import load_dotenv\n",
    "from gtts import gTTS\n",
    "import tempfile\n",
    "import whisper\n",
    "import ollama\n",
    "import matplotlib.pyplot as plt\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + r\"C:/ffmpeg/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
    "    try:\n",
    "        transcribed_text = transcribe_audio(audio)\n",
    "        return transcribed_text\n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, split, and retrieve documents\n",
    "\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"gemma2\")\n",
    "\n",
    "    # vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format documents\n",
    "\n",
    "def format_docs(docs):\n",
    "\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that defines the RAG chain\n",
    "\n",
    "def rag_chain(url, question):\n",
    "\n",
    "    retriever = load_and_retrieve_docs(url)\n",
    "\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "\n",
    "    response = ollama.chat(model='gemma2', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(text):\n",
    "    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='malgun',\n",
    "        background_color='white',\n",
    "        width=800,\n",
    "        height=600,\n",
    "        max_words=200,\n",
    "        max_font_size=100,\n",
    "        min_font_size=10,\n",
    "        random_state=42\n",
    "    ).generate(text)\n",
    "    \n",
    "    # í”Œë¡¯ ìƒì„± ë° figure ê°ì²´ ë°˜í™˜\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gradio interface\n",
    "\n",
    "# iface = gr.Interface(\n",
    "\n",
    "#     fn=rag_chain,\n",
    "\n",
    "#     inputs=[\"text\", \"text\"],\n",
    "\n",
    "#     outputs=\"text\",\n",
    "\n",
    "#     title=\"RAG Chain Question Answering\",\n",
    "\n",
    "#     description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_1732\\2629661401.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"gemma2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # ë””ë²„ê·¸ ëª¨ë“œë¡œ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "# iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get text from URL\n",
    "def get_text_from_url(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    return \" \".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang='ko'):\n",
    "    # ì„ì‹œ íŒŒì¼ ìƒì„±\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:\n",
    "        temp_filename = fp.name\n",
    "\n",
    "    # TTS ë³€í™˜\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ì¬ìƒ ë˜ëŠ” ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Tabbed Interface\n",
    "with gr.Blocks() as iface:\n",
    "    # Tab for Question and Answer\n",
    "    with gr.Tab(\"ì§ˆë¬¸ê³¼ ë‹µë³€\"):\n",
    "        gr.Interface(\n",
    "            fn=rag_chain,\n",
    "            inputs=[\"text\", \"text\"],\n",
    "            outputs=\"text\",\n",
    "            title=\"RAG Chain Question Answering\",\n",
    "            description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "        ).render()\n",
    "\n",
    "    with gr.Tab(\"ì‹œê°í™” (ì›Œë“œí´ë¼ìš°ë“œ)\"):\n",
    "        with gr.Row():\n",
    "            text_input = gr.Textbox(\n",
    "                label=\"URL ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥\",\n",
    "                placeholder=\"URLì´ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”\",\n",
    "                lines=5\n",
    "            )\n",
    "        \n",
    "        def process_input(input_text):\n",
    "            if input_text.startswith(('http://', 'https://')):\n",
    "                text = get_text_from_url(input_text)\n",
    "            else:\n",
    "                text = input_text\n",
    "            return generate_wordcloud(text)\n",
    "        \n",
    "        plot_output = gr.Plot()  # Plot ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©\n",
    "        generate_btn = gr.Button(\"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\")\n",
    "        \n",
    "        generate_btn.click(\n",
    "            fn=process_input,\n",
    "            inputs=text_input,\n",
    "            outputs=plot_output\n",
    "        )\n",
    "    with gr.Tab(\"ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"):\n",
    "        gr.Interface(\n",
    "            fn=process_audio,\n",
    "            inputs=gr.Audio(type=\"filepath\", label=\"MP3 íŒŒì¼ ì—…ë¡œë“œ\"),\n",
    "            outputs = \"text\",\n",
    "            title = \"MP3 to Text Converter\",\n",
    "            description = \"MP3 íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\n",
    "        )\n",
    "    with gr.Tab(\"í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜\"):\n",
    "        gr.Interface(\n",
    "            fn=process_tts,\n",
    "            inputs=[\n",
    "                gr.Textbox(lines=5, label=\"í…ìŠ¤íŠ¸ ì…ë ¥\"),\n",
    "                gr.Dropdown(choices=['ko','en','ja','zh-cn'], label=\"ì–¸ì–´ ì„ íƒ\", value='ko')\n",
    "            ],\n",
    "            outputs=[\n",
    "                gr.Audio(label=\"ìƒì„±ëœ ì˜¤ë””ì˜¤\"),\n",
    "                gr.Textbox(label=\"ìƒíƒœ ë©”ì‹œì§€\")\n",
    "            ],\n",
    "            title = \"Text to Speech Converter\",\n",
    "            description = \"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ MP3íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë””ë²„ê·¸ ëª¨ë“œë¡œ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(STT:ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ì „í™˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import whisper\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffmpeg ê²½ë¡œ ëª…ì‹œì  ì„¤ì •\n",
    "# os.environ[\"FFMPEG_BINARY\"] = \"C:/aiproject/ffmpeg/bin/ffmpeg.exe\"\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:/ffmpeg/bin\"\n",
    "# os.environ[\"FFMPEG_BINARY\"] = r\"C:/Ai_Project/ffmpeg/bin/ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    # Whisper ëª¨ë¸ ë¡œë“œ\n",
    "    model = whisper.load_model(\"base\")\n",
    "    \n",
    "    # ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬\n",
    "    result = model.transcribe(audio_path)\n",
    "    \n",
    "    # ì „ì‚¬ëœ í…ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\"\n",
    "    try:\n",
    "        transcribed_text = transcribe_audio(audio)\n",
    "        return transcribed_text\n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio, \n",
    "    inputs = gr.Audio(type=\"filepath\", label=\"MP3 íŒŒì¼ ì—…ë¡œë“œ\"),\n",
    "    outputs=\"text\", \n",
    "    title = \"MP3 to Text Converter\",\n",
    "    description = \"MP3 íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë””ë²„ê·¸ ëª¨ë“œë¡œ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì˜ˆì œ(TTS:í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ë³€í™˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ì „ ì„¤ì¹˜ : pip install gtts\n",
    "import gradio as gr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang='ko'):\n",
    "    # ì„ì‹œ íŒŒì¼ ìƒì„±\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:\n",
    "        temp_filename = fp.name\n",
    "    \n",
    "    # TTS ë³€í™˜\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ì¬ìƒ ë˜ëŠ” ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "iface = gr.Interface(\n",
    "    fn=process_tts, \n",
    "    inputs = [\n",
    "        gr.Textbox(lines=5, label=\"í…ìŠ¤íŠ¸ ì…ë ¥\"),\n",
    "        gr.Dropdown(choices=['ko','en','ja','zh-cn'], label=\"ì–¸ì–´ ì„ íƒ\", value='ko')\n",
    "    ], \n",
    "    outputs=[\n",
    "        gr.Audio(label=\"ìƒì„ ëœ ì˜¤ë””ì˜¤\"),\n",
    "        gr.Textbox(label=\"ìƒíƒœ ë©”ì‹œì§€\")\n",
    "    ],\n",
    "    title=\"Text to Speech Converter\",\n",
    "    description=\"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ MP3 íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë””ë²„ê·¸ ëª¨ë“œë¡œ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBì ‘ì† ì±—ë´‡ ì˜ˆì œ(Gradio ì‚¬ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import re\n",
    "from typing import Dict, Any\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB ì—°ê²° ì„¤ì •\n",
    "DB_URL = \"mysql+pymysql://root:00000000@localhost:3306/test\"\n",
    "engine = create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedQueryGenerator:\n",
    "    \"\"\"í–¥ìƒëœ SQL ì¿¼ë¦¬ ìƒì„± í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.query_template = \"\"\"\n",
    "        ë‹¹ì‹ ì€ í•œêµ­ì–´ë¥¼ ì˜í•˜ê³  MySQL ë°ì´í„°ë² ì´ìŠ¤ì˜ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "        ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´:\n",
    "        {schema_info}\n",
    "\n",
    "        ì´ì „ í”¼ë“œë°± ì •ë³´:\n",
    "        {feedback_info}\n",
    "\n",
    "        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ MySQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "        ì§ˆë¬¸: {question}\n",
    "\n",
    "        ê·œì¹™:\n",
    "        1. ìˆœìˆ˜í•œ SQL ì¿¼ë¦¬ë§Œ ì‘ì„±í•˜ì„¸ìš”\n",
    "        2. ì»¬ëŸ¼ì˜ ì‹¤ì œ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
    "        3. ì„¤ëª…ì´ë‚˜ ì£¼ì„ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\n",
    "        4. ì¿¼ë¦¬ëŠ” SELECT ë¬¸ìœ¼ë¡œ ì‹œì‘í•˜ê³  ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤\n",
    "        5. WHERE ì ˆì—ì„œëŠ” ì •í™•í•œ ê°’ ë§¤ì¹­ì„ ìœ„í•´ = ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "        6. ìœ ì‚¬ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° LIKE '%í‚¤ì›Œë“œ%' ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "        7. ê´€ë ¨ëœ ëª¨ë“  ê²°ê³¼ë¥¼ ì°¾ê¸° ìœ„í•´ ì ì ˆíˆ OR ì¡°ê±´ì„ í™œìš©í•˜ì„¸ìš”\n",
    "        \"\"\"\n",
    "\n",
    "        self.answer_template = \"\"\"\n",
    "        ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:\n",
    "\n",
    "        ì›ë˜ ì§ˆë¬¸: {question}\n",
    "        ì‹¤í–‰ëœ ì¿¼ë¦¬: {query}\n",
    "        ì¿¼ë¦¬ ê²°ê³¼: {result}\n",
    "\n",
    "        ê·œì¹™:\n",
    "        1. ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”\n",
    "        2. ìˆ«ì ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì ì ˆí•œ ë‹¨ìœ„ì™€ í•¨ê»˜ í‘œí˜„í•´ì£¼ì„¸ìš”\n",
    "        3. ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n",
    "        4. ì „ë¬¸ì ì¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”\n",
    "        \"\"\"\n",
    "\n",
    "        # Gemma2 ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.llm = Ollama(\n",
    "            model=\"gemma2\",\n",
    "            temperature=0,\n",
    "            callback_manager=callback_manager\n",
    "        )\n",
    "\n",
    "        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "        self.query_prompt = ChatPromptTemplate.from_template(self.query_template)\n",
    "        self.answer_prompt = ChatPromptTemplate.from_template(self.answer_template)\n",
    "\n",
    "        # Chain ì„¤ì •\n",
    "        self.query_chain = LLMChain(llm=self.llm, prompt=self.query_prompt)\n",
    "        self.answer_chain = LLMChain(llm=self.llm, prompt=self.answer_prompt)\n",
    "\n",
    "    def generate_query(self, question: str, schema_info: str, feedback_info: str = \"\") -> str:\n",
    "        \"\"\"ì§ˆë¬¸ì— ëŒ€í•œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "        response = self.query_chain.run(\n",
    "            question=question,\n",
    "            schema_info=schema_info,\n",
    "            feedback_info=feedback_info\n",
    "        )\n",
    "        return self.extract_sql_query(response)\n",
    "\n",
    "    def generate_answer(self, question: str, query: str, result: Any) -> str:\n",
    "        \"\"\"ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ì–´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "        result_str = str(result) if isinstance(result, pd.DataFrame) else json.dumps(result, ensure_ascii=False)\n",
    "        response = self.answer_chain.run(\n",
    "            question=question,\n",
    "            query=query,\n",
    "            result=result_str\n",
    "        )\n",
    "        return response.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_sql_query(response: str) -> str:\n",
    "        \"\"\"ì‘ë‹µì—ì„œ SQL ì¿¼ë¦¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        response = response.replace('```sql', '').replace('```', '').strip()\n",
    "        match = re.search(r'SELECT.*?;', response, re.DOTALL | re.IGNORECASE)\n",
    "        return match.group(0).strip() if match else response.strip()\n",
    "\n",
    "# ì¿¼ë¦¬ ê²°ê³¼ ë°˜í™˜\n",
    "def get_schema_info():\n",
    "    \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        tables = pd.read_sql(\"SHOW TABLES\", conn)\n",
    "        schema_info = []\n",
    "\n",
    "        for table in tables.iloc[:, 0]:\n",
    "            columns = pd.read_sql(f\"DESCRIBE {table}\", conn)\n",
    "            schema_info.append(f\"í…Œì´ë¸”: {table}\")\n",
    "            schema_info.append(\"ì»¬ëŸ¼:\")\n",
    "            for _, row in columns.iterrows():\n",
    "                schema_info.append(f\"- {row['Field']} ({row['Type']})\")\n",
    "            schema_info.append(\"\")\n",
    "\n",
    "        return \"\\n\".join(schema_info)\n",
    "\n",
    "def execute_query(query):\n",
    "    \"\"\"SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            result = pd.read_sql(query, conn)\n",
    "            return result\n",
    "    except Exception as e:\n",
    "        return f\"ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "def process_question(question):\n",
    "    \"\"\"ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    schema_info = get_schema_info()\n",
    "    query_generator = EnhancedQueryGenerator()\n",
    "\n",
    "    # ì¿¼ë¦¬ ìƒì„±\n",
    "    query = query_generator.generate_query(question, schema_info)\n",
    "\n",
    "    # ì¿¼ë¦¬ ì‹¤í–‰\n",
    "    result = execute_query(query)\n",
    "\n",
    "    # ë‹µë³€ ìƒì„±\n",
    "    answer = query_generator.generate_answer(question, query, result)\n",
    "\n",
    "    return query, result, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "def create_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# DB ë¬¸ì˜ ì±—ë´‡ (Gemma2 ê¸°ë°˜)\")\n",
    "\n",
    "        with gr.Row():\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”\",\n",
    "                placeholder=\"ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”...\"\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"ì§ˆë¬¸í•˜ê¸°\")\n",
    "\n",
    "        with gr.Row():\n",
    "            query_output = gr.Textbox(label=\"ìƒì„±ëœ SQL ì¿¼ë¦¬\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                result_output = gr.Dataframe(label=\"ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼\")\n",
    "\n",
    "        with gr.Row():\n",
    "            answer_output = gr.Textbox(\n",
    "                label=\"AI ë‹µë³€\",\n",
    "                lines=5\n",
    "            )\n",
    "\n",
    "        submit_btn.click(\n",
    "            fn=process_question,\n",
    "            inputs=[question_input],\n",
    "            outputs=[query_output, result_output, answer_output]\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://1726dcbd9b81d7994f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1726dcbd9b81d7994f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7861 <> https://1726dcbd9b81d7994f.gradio.live\n"
     ]
    }
   ],
   "source": [
    "# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_interface()\n",
    "    demo.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ìê¸°ì†Œê°œì„œ ë„ìš°ë¯¸ ì±—ë´‡ ì˜ˆì œ(Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_4824\\1062899569.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  ollama_model = Ollama(model = \"gemma2\")\n"
     ]
    }
   ],
   "source": [
    "# Ollama ì„¤ì • (Gemma2 ëª¨ë¸ ì‚¬ìš©)\n",
    "os.environ[\"OLLAMA_API_BASE\"] = \"http://localhost:11434\" # Ollama ì„œë²„ ì£¼ì†Œ\n",
    "ollama_model = Ollama(model = \"gemma2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì–‘í•œ í…œí”Œë¦¿ ì„¤ì •\n",
    "TEMPLATES = {\n",
    "    \"ì·¨ì—…\": \"Based on the following keywords and example, write a personal statement for a job application.\",\n",
    "    \"ëŒ€í•™ì›\": \"Using the provided keywords, draft a personal statement for a graduate school application.\",\n",
    "    \"ë´‰ì‚¬í™œë™\": \"With the given keywords, write a personal statement emphasizing volunteer experience and motivation.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–¸ì–´ ì§€ì›: í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´\n",
    "LANGUAGES = {\n",
    "    \"í•œêµ­ì–´\": \"Please write the response in Korean.\",\n",
    "    \"ì˜ì–´\": \"Please write the response in English.\",\n",
    "    \"ì¼ë³¸ì–´\": \"Please write the response in Japanese.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìë™ í‚¤ì›Œë“œ ì¶”ì²œ í•¨ìˆ˜\n",
    "def recommend_keywords(purpose):\n",
    "    if purpose == \"ì·¨ì—…\":\n",
    "        return \"ì±…ì„ê°, íŒ€ì›Œí¬, ë¬¸ì œ í•´ê²° ëŠ¥ë ¥\"\n",
    "    elif purpose == \"ëŒ€í•™ì›\":\n",
    "        return \"ì—°êµ¬ ì—´ì •, ì°½ì˜ë ¥, í•™ì—… ì„±ì·¨ë„\"\n",
    "    elif purpose == \"ë´‰ì‚¬í™œë™\":\n",
    "        return \"ì‚¬íšŒì  ì±…ì„ê°, í¬ìƒì •ì‹ , ë¦¬ë”ì‹­\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìê¸°ì†Œê°œì„œ ì‘ì„± í•¨ìˆ˜\n",
    "def generate_statement(purpose, language, keywords, example_sentence=None):\n",
    "    if purpose not in TEMPLATES:\n",
    "        return \"âŒ ì§€ì› ëª©ì ì„ ì˜¬ë°”ë¥´ê²Œ ì„ íƒí•´ì£¼ì„¸ìš”.\"\n",
    "    if language not in LANGUAGES:\n",
    "        return \"âŒ ì–¸ì–´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„ íƒí•´ì£¼ì„¸ìš”.\"\n",
    "    \n",
    "    # í…œí”Œë¦¿ ìƒì„±\n",
    "    template = TEMPLATES[purpose] + \"\\n\\nKeywords: {keywords}\\n\" + LANGUAGES[language]\n",
    "    if example_sentence:\n",
    "        template += f\"\\n\\nExample sentence: {example_sentence}\"\n",
    "        \n",
    "    prompt =PromptTemplate(input_variables=[\"keywords\"], template = template)\n",
    "    chain = LLMChain(llm=ollama_model, prompt=prompt)\n",
    "    response = chain.run({\"keywords\": keywords})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF ì €ì¥ í•¨ìˆ˜\n",
    "def save_to_pdf(statement, filename=\"personal_statement.pdf\"):\n",
    "    pdf=FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.add_font('MalgunGothic', '', r'C:\\Windows\\Fonts\\malgun.ttf', uni=True) # 'ë§‘ì€ ê³ ë”•' í°íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "    pdf.set_font('MalgunGothic', size=12) # í°íŠ¸ ì„¤ì •\n",
    "    pdf.multi_cell(0,10,statement)\n",
    "    pdf.output(filename)\n",
    "    return f\"âœ”ï¸ PDF ì €ì¥ ì™„ë£Œ: {filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤\n",
    "def chatbot_interface(purpose, language, keywords, example_sentence=None, save_pdf=False):\n",
    "    statement = generate_statement(purpose, language, keywords, example_sentence)\n",
    "    if save_pdf:\n",
    "        save_to_pdf(statement)\n",
    "    return statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ğŸ“ ë‹¤ëª©ì  ìê¸°ì†Œê°œì„œ ì‘ì„± ë„ìš°ë¯¸\")\n",
    "    gr.Markdown(\"í‚¤ì›Œë“œì™€ ì¶”ì²œ ë¬¸ì¥ì„ í™œìš©í•˜ì—¬ ì·¨ì—…, ëŒ€í•™ì›, ë´‰ì‚¬í™œë™ ìê¸°ì†Œê°œì„œë¥¼ ìƒì„±í•˜ê³  PDFë¡œ ì €ì¥í•˜ì„¸ìš”!\")\n",
    "    \n",
    "    # ì…ë ¥ ì˜ì—­\n",
    "    with gr.Row():\n",
    "        purpose_input = gr.Dropdown(label=\"ì§€ì› ëª©ì \", choices=[\"ì·¨ì—…\",\"ëŒ€í•™ì›\",\"ë´‰ì‚¬í™œë™\"], value=\"ì·¨ì—…\")\n",
    "        language_input = gr.Dropdown(label=\"ì–¸ì–´ ì„ íƒ\", choices=[\"í•œêµ­ì–´\", \"ì˜ì–´\", \"ì¼ë³¸ì–´\"], value=\"í•œêµ­ì–´\")\n",
    "        \n",
    "    recommended_keywords = gr.Textbox(label=\"ì¶”ì²œ í‚¤ì›Œë“œ\", interactive=False)\n",
    "    recommend_btn = gr.Button(\"í‚¤ì›Œë“œ ì¶”ì²œ\")\n",
    "    recommend_btn.click(recommend_keywords, inputs=[purpose_input], outputs=[recommended_keywords])\n",
    "    \n",
    "    with gr.Row():\n",
    "        keywords_input = gr.Textbox(label=\"ì‚¬ìš©ì í‚¤ì›Œë“œ ì…ë ¥\", placeholder=\"ì˜ˆ: ì±…ì„ê°, íŒ€ì›Œí¬, ë¬¸ì œ í•´ê²° ëŠ¥ë ¥\")\n",
    "        example_sentence_input = gr.Textbox(\n",
    "            label=\"ì¶”ì²œ ë¬¸ì¥ (ì„ íƒ ì‚¬í•­)\", \n",
    "            placeholder=\"ì˜ˆ: 'ì €ëŠ” ë„ì „ì„ ë‘ë ¤ì›Œí•˜ì§€ ì•Šê³  ì„±ê³µì ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ ì™„ìˆ˜í–ˆìŠµë‹ˆë‹¤.'\"\n",
    "        )\n",
    "    save_pdf_toggle=gr.Checkbox(label=\"PDFë¡œ ì €ì¥\", value=False)\n",
    "    \n",
    "    # ì¶œë ¥ ì˜ì—­\n",
    "    output = gr.Textbox(label=\"ì‘ì„±ëœ ìê¸°ì†Œê°œì„œ\", lines=6)\n",
    "    submit_btn = gr.Button(\"ì‘ì„±í•˜ê¸°\")\n",
    "    submit_btn.click(\n",
    "        fn=chatbot_interface, \n",
    "        inputs=[purpose_input, language_input, keywords_input, example_sentence_input, save_pdf_toggle],\n",
    "        outputs=[output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
