{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install ollama\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "import warnings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import gradio as gr\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = None\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2\")\n",
    "llm = ChatOllama(model = \"gemma2\", temperature=0.6, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    \"\"\"파일을 처리하고 QA 체인을 설정하는 함수\"\"\"\n",
    "    global qa_chain\n",
    "    \n",
    "    try:\n",
    "        # 직접 파일 경로를 TextLoader에 전달\n",
    "        loader = TextLoader(file_path, encoding='UTF8')\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # 벡터 저장소 생성\n",
    "        vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "        \n",
    "        # QA 체인 설정\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=retriever\n",
    "        )\n",
    "        \n",
    "        return \"파일이 성공적으로 처리되었습니다.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"파일 처리 중 오류가 발생했습니다: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \"\"\"챗봇 응답을 생성하는 함수\"\"\"\n",
    "    global qa_chain\n",
    "    \n",
    "    if qa_chain is None:\n",
    "        return \"먼저 파일을 업로드해 주세요.\"\n",
    "    \n",
    "    try:\n",
    "        # QA 체인으로 응답 생성\n",
    "        response = qa_chain.run(message)\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"응답 생성 중 오류가 발생했습니다: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Ai_Project\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# RAG Chatbot\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_output = gr.Textbox(label=\"파일 처리 상태\")\n",
    "        upload_button = gr.File(\n",
    "            label=\"텍스트 파일 업로드\",\n",
    "            file_types=[\".txt\"]\n",
    "        )\n",
    "    \n",
    "    chatbot_interface = gr.ChatInterface(\n",
    "        chat,\n",
    "        examples=[\n",
    "            \"고조선은 언제 설립되었나요?\",\n",
    "            \"삼국시대의 주요 사건은 무엇인가요?\",\n",
    "            \"조선시대의 대표적인 문화유산은 무엇인가요?\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    upload_button.upload(\n",
    "        fn=process_file,\n",
    "        inputs=[upload_button],\n",
    "        outputs=[file_output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-14\\AppData\\Local\\Temp\\ipykernel_13256\\4268912487.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(message)\n"
     ]
    }
   ],
   "source": [
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
